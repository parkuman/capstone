{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import pyaudio\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel Frequency Cepstral Coefficients (MFCC)\n",
    "def extract_mfcc(y, sr):\n",
    "  mfcc = np.array(librosa.feature.mfcc(y=y, sr=sr))\n",
    "  return mfcc\n",
    "\n",
    "def extract_melspectrogram(y, sr):\n",
    "  melspectrogram = np.array(librosa.feature.melspectrogram(y=y, sr=sr))\n",
    "  return melspectrogram\n",
    "\n",
    "def extract_chroma_vector(y, sr):\n",
    "  chroma = np.array(librosa.feature.chroma_stft(y=y, sr=sr))\n",
    "  return chroma\n",
    "\n",
    "def extract_spectral_contrast(y, sr):\n",
    "  tonnetz = np.array(librosa.feature.spectral_contrast(y=y, sr=sr))\n",
    "  return tonnetz\n",
    "\n",
    "# tonal centroid features (tonnetz)\n",
    "def extract_tonnetz(y, sr):\n",
    "  tonnetz = np.array(librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr))\n",
    "  return tonnetz\n",
    "\n",
    "def extract_features(y, sr):\n",
    "  # Extracting MFCC feature\n",
    "  mfcc = extract_mfcc(y, sr)\n",
    "  mfcc_mean = mfcc.mean(axis=1)\n",
    "  mfcc_min = mfcc.min(axis=1)\n",
    "  mfcc_max = mfcc.max(axis=1)\n",
    "  mfcc_feature = np.concatenate( (mfcc_mean, mfcc_min, mfcc_max) )\n",
    "\n",
    "  # # Extracting Mel Spectrogram feature\n",
    "  # melspectrogram = extract_melspectrogram(y, sr)\n",
    "  # melspectrogram_mean = melspectrogram.mean(axis=1)\n",
    "  # melspectrogram_min = melspectrogram.min(axis=1)\n",
    "  # melspectrogram_max = melspectrogram.max(axis=1)\n",
    "  # melspectrogram_feature = np.concatenate( (melspectrogram_mean, melspectrogram_min, melspectrogram_max) )\n",
    "\n",
    "  # # Extracting chroma vector feature\n",
    "  # chroma = extract_chroma_vector(y, sr)\n",
    "  # chroma_mean = chroma.mean(axis=1)\n",
    "  # chroma_min = chroma.min(axis=1)\n",
    "  # chroma_max = chroma.max(axis=1)\n",
    "  # chroma_feature = np.concatenate( (chroma_mean, chroma_min, chroma_max) )\n",
    "\n",
    "  # # Extracting spectral contrast feature\n",
    "  # spectral_contrast = extract_spectral_contrast(y, sr)\n",
    "  # spectral_contrast_mean = spectral_contrast.mean(axis=1)\n",
    "  # spectral_contrast_min = spectral_contrast.min(axis=1)\n",
    "  # spectral_contrast_max = spectral_contrast.max(axis=1)\n",
    "  # spectral_contrast_feature = np.concatenate( (spectral_contrast_mean, spectral_contrast_min, spectral_contrast_max) )\n",
    "\n",
    "  # # Extracting tonnetz feature\n",
    "  # tonnetz = extract_tonnetz(y, sr)\n",
    "  # tonnetz_mean = tonnetz.mean(axis=1)\n",
    "  # tonnetz_min = tonnetz.min(axis=1)\n",
    "  # tonnetz_max = tonnetz.max(axis=1)\n",
    "  # tonnetz_feature = np.concatenate( (tonnetz_mean, tonnetz_min, tonnetz_max) ) \n",
    "  \n",
    "  # features = np.concatenate( (melspectrogram_feature, mfcc_feature, chroma_feature, spectral_contrast_feature, tonnetz_feature) )\n",
    "  features = mfcc_feature \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,11):\n",
    "#   file_path = f\"./training_data/parker/recording_{i}.m4a\"\n",
    "\n",
    "\n",
    "#   mfcc = extract_mfcc(y, sr)\n",
    "#   pyplot.imshow(mfcc, interpolation='nearest', aspect='auto')\n",
    "#   pyplot.title(\"mfcc\\n\" + file_path)\n",
    "#   pyplot.show()\n",
    "\n",
    "#   melspectrogram = extract_melspectrogram(y, sr)\n",
    "#   pyplot.imshow(melspectrogram, interpolation='nearest', aspect='auto')\n",
    "#   pyplot.title(\"melspectrogram\\n\" + file_path)\n",
    "#   pyplot.show()\n",
    "\n",
    "#   chroma_vector = extract_chroma_vector(y, sr)\n",
    "#   pyplot.imshow(chroma_vector, interpolation='nearest', aspect='auto')\n",
    "#   pyplot.title(\"chroma_vector\\n\" + file_path)\n",
    "#   pyplot.show()\n",
    "\n",
    "#   spectral_contrast = extract_spectral_contrast(y, sr)\n",
    "#   pyplot.imshow(chroma_vector, interpolation='nearest', aspect='auto')\n",
    "#   pyplot.title(\"spectral_contrast\\n\" + file_path)\n",
    "#   pyplot.show()\n",
    "\n",
    "#   tonnetz = extract_tonnetz(y, sr)\n",
    "#   pyplot.imshow(tonnetz, interpolation='nearest', aspect='auto')\n",
    "#   pyplot.title(\"tonnetz\\n\" + file_path)\n",
    "#   pyplot.show()\n",
    "\n",
    "\n",
    "directory = \"training_data_wav\"\n",
    "speakers = os.listdir(directory)\n",
    "features = []\n",
    "labels = []\n",
    "for speaker in speakers:\n",
    "    print(\"=================================| Extracting features for speaker: \" + speaker + \" |=================================\")\n",
    "    for file in os.listdir(directory + \"/\" + speaker):\n",
    "        file_path = directory + \"/\" + speaker + \"/\" + file\n",
    "        print(\"[\" + file_path + \"]\\n Splitting up audio and extracting features\", end=\"\")\n",
    "\n",
    "        sr = librosa.get_samplerate(file_path)\n",
    "        stream = librosa.stream(file_path, # 2 seconds\n",
    "                            block_length=256,\n",
    "                            frame_length=2048,\n",
    "                            hop_length=512)\n",
    "\n",
    "        label = speakers.index(speaker)\n",
    "\n",
    "        for y in stream:\n",
    "            print(\".\", end=\"\")\n",
    "            features.append(extract_features(y, sr))\n",
    "            labels.append(label)\n",
    "            \n",
    "        print(\" Done!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_audio_samples = len(features)\n",
    "num_classes = len(speakers)\n",
    "\n",
    "# randomize our data each time so we aren't training on the exact same set of data\n",
    "permutations = np.random.permutation(num_audio_samples)\n",
    "features = np.array(features)[permutations]\n",
    "labels = np.array(labels)[permutations]\n",
    "\n",
    "# one hot encode our labels\n",
    "labels_one_hot = tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# data splits\n",
    "train_split = 0.8\n",
    "test_split = 0.1\n",
    "val_split = 0.1\n",
    "\n",
    "if train_split + test_split + val_split != 1.0:\n",
    "  raise ValueError(\"test, train and validation splits should sum to 1.0 (100%)\")\n",
    "\n",
    "# split our data using numpy split. by giving two indices the data is split three ways:\n",
    "#  - everything before index 1 will be our training data\n",
    "#  - everything after index 1 and until index 2 will be our test data\n",
    "#  - everything after index 2 will be our validation data\n",
    "features_train, features_test, features_val = np.split(features, [int(num_audio_samples * train_split), int(num_audio_samples * (train_split + test_split))])\n",
    "labels_train, labels_test, labels_val = np.split(labels_one_hot, [int(num_audio_samples * train_split), int(num_audio_samples * (train_split + test_split))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the amounts of each class to see if we have roughly the same amount of data for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, class_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "print(class_counts)\n",
    "print(\"Class Counts:\")\n",
    "for i in range(len(speakers)):\n",
    "  print(f\"{speakers[i]} - {class_counts[i]}\")\n",
    "print(\"\\nmin - \" + str(np.min(class_counts)))\n",
    "print(\"max - \" + str(np.max(class_counts)))\n",
    "print(\"standard deviation - \" + str(np.std(class_counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_train.shape)\n",
    "print(labels_train.shape)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Dense(features_train.shape[1], input_shape=(features_train.shape[1],), activation = \"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256, activation = \"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.25))  \n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, activation = \"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.25))  \n",
    "\n",
    "model.add(tf.keras.layers.Dense(64, activation = \"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))  \n",
    "  \n",
    "\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "lr_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                 patience=4, \n",
    "                                 verbose=2, \n",
    "                                 factor=.75)\n",
    "\n",
    "# If score doesn't improve, stop learning\n",
    "estopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                          patience=5, \n",
    "                          verbose=2)\n",
    "\n",
    "\n",
    "history = model.fit(features_train, labels_train, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    validation_data=(features_val, labels_val),\n",
    "                    callbacks=[\n",
    "                      lr_reduction, \n",
    "                      estopping\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our train accuracy and validation accuracy over epochs.\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_accuracy, label='Training Accuracy', color='#185fad')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy', color='orange')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(0,EPOCHS,5), range(0,EPOCHS,5))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = np.argmax(model.predict(features_test), axis=-1)\n",
    "# print(np.argmax(labels_test, axis=-1))\n",
    "# print(preds)\n",
    "score = model.evaluate(x=features_test.tolist(),y=labels_test.tolist(), verbose=1)\n",
    "print('Accuracy : ' + str(score[1]*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # whole file\n",
    "# y, sr = librosa.load(directory + \"/parker/recording_14.wav\", offset=0, duration=30)\n",
    "\n",
    "# frames of a file\n",
    "test_file_path = \"testing_data/adam/jan_17.wav\"\n",
    "sr = librosa.get_samplerate(test_file_path)\n",
    "stream = librosa.stream(test_file_path,\n",
    "                    block_length=256,\n",
    "                    frame_length=2048,\n",
    "                    hop_length=512)\n",
    "\n",
    "# microphone input\n",
    "# sr = 44100\n",
    "# pa = pyaudio.PyAudio()\n",
    "# print(pa.get_device_count())\n",
    "# pa.get_default_output_device_info()\n",
    "# stream = pa.open(format=pyaudio.paInt16, channels=1, rate=sr, input=True)\n",
    "# stream = librosa.stream(stream,\n",
    "#                          block_length=256,\n",
    "#                          frame_length=2048,\n",
    "#                          hop_length=2048)\n",
    "\n",
    "for y in stream:\n",
    "    test_frame_features = extract_features(y, sr)\n",
    "    pred = model.predict(test_frame_features.reshape(1,len(test_frame_features)))\n",
    "    idx = np.argmax(pred)\n",
    "    print(pred)\n",
    "    print(idx)\n",
    "    print(speakers[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"jan_15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a0b5d9a015021ea0ae4291e9f6e6173059728ea7e59993f1dba0913535750ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
